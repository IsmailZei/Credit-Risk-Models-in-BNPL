{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pointbiserialr\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OutlierDetector(data):\n",
    "    '''Detects outliers in a given pandas Series using the IQR method.\n",
    "    Args:\n",
    "        data (pd.Series): The input data series to check for outliers.\n",
    "    Returns:\n",
    "        pd.Series: A series containing the outliers.\n",
    "    Raises:\n",
    "        ValueError: If the input data is not a pandas Series.   \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    q1 = data.quantile(0.25)\n",
    "    q3 = data.quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    lower, upper = q1 - 1.5*iqr, q3 + 1.5*iqr\n",
    "    return data[(data < lower) | (data > upper)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DefaultFlagGenerator(df):\n",
    "    \"\"\"Generates a default flag based on multiple financial stress indicators.\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing financial indicators.\n",
    "            Must include the following columns:\n",
    "            - 'payment_delinquency_count'\n",
    "            - 'over_indebtedness_flag'\n",
    "            - 'financial_stress_score'\n",
    "            - 'bnpl_debt_ratio'\n",
    "            - 'credit_limit_utilisation'\n",
    "    Returns:\n",
    "        pd.DataFrame: The original DataFrame with an additional column 'default_flag'.\n",
    "            This column is 1 if the customer is likely to default, 0 otherwise.\n",
    "    \"\"\"\n",
    "    default_flag = (\n",
    "        (df['payment_delinquency_count'] >= 3).astype(int) +\n",
    "        (df['over_indebtedness_flag'] == 1).astype(int) +\n",
    "        (df['financial_stress_score'] >= 9).astype(int) +\n",
    "        (df['bnpl_debt_ratio'] >= 1.8).astype(int) +\n",
    "        (df['credit_limit_utilisation'] >= 95).astype(int)\n",
    "        ) >= 3 # Must meet at least 3 of the 5 conditions\n",
    "    df['default_flag'] = default_flag.astype(int)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_col(data):\n",
    "    \"\"\"Identifies binary columns in a DataFrame.\n",
    "    Args:\n",
    "        data (pd.DataFrame): The input DataFrame to check for binary columns.\n",
    "        Returns:\n",
    "        list: A list of column names that are binary (i.e., have exactly two unique values).\n",
    "        Raises:\n",
    "            ValueError: If the input data is not a pandas DataFrame.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    binary_cols=[col for col in data.columns if data[col].nunique() == 2]\n",
    "    return binary_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def continuous_col(data):\n",
    "    \"\"\" Identifies continuous columns in a DataFrame.\n",
    "    Args:\n",
    "        data (pd.DataFrame): The input DataFrame to check for continuous columns.\n",
    "    Returns:\n",
    "        list: A list of column names that are continuous (i.e., have more than two unique values).\n",
    "    Raises:\n",
    "        ValueError: If the input data is not a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    continuous_cols=[col for col in data.columns if data[col].nunique() > 2]\n",
    "    return continuous_cols\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Scaler(data):\n",
    "\n",
    "    \"\"\"Scales continuous columns in a DataFrame using StandardScaler.\n",
    "    Args:\n",
    "        data (pd.DataFrame): The input DataFrame to scale.\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with binary columns unchanged and continuous columns scaled.\n",
    "    Raises:\n",
    "        ValueError: If the input data is not a pandas DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    binary_cols = binary_col(data)\n",
    "    continuous_cols = continuous_col(data)\n",
    "    scaler = StandardScaler()\n",
    "   # Scale the continuous columns\n",
    "    scaled_continuous = scaler.fit_transform(data[continuous_cols])\n",
    "    scaled_continuous_df = pd.DataFrame(scaled_continuous, columns=continuous_cols)\n",
    "    # Combine the scaled continuous columns with the binary columns\n",
    "    result = pd.concat([data[binary_cols].reset_index(drop=True), scaled_continuous_df.reset_index(drop=True)], axis=1)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads a CSV from the given path and prints its .info().\n",
    "    Returns the loaded DataFrame.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    df.info()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Tables/BNPL.csv')\n",
    "data = data.select_dtypes(include=['number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect outliers in each feature column\n",
    "for column in data.select_dtypes(include=['float64', 'int64']).columns:\n",
    "    outliers = OutlierDetector(data[column])\n",
    "    print(f\"Outliers in {column}:\\n\", outliers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate proxy default flag\n",
    "data_clean = DefaultFlagGenerator(data)\n",
    "data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Scaler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m data_scaled = \u001b[43mScaler\u001b[49m(data_clean)\n\u001b[32m      2\u001b[39m data_scaled.to_csv(\u001b[33m'\u001b[39m\u001b[33mTables/bnpl_scaled.csv\u001b[39m\u001b[33m'\u001b[39m, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mData processing complete. Scaled data saved to \u001b[39m\u001b[33m'\u001b[39m\u001b[33mbnpl_scaled.csv\u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'Scaler' is not defined"
     ]
    }
   ],
   "source": [
    "# Scale the data\n",
    "data_scaled = Scaler(data_clean)\n",
    "data_scaled.to_csv('Tables/bnpl_scaled.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the continuous and binary columns\n",
    "continuous_cols = continuous_col(data_scaled)\n",
    "binary_cols = binary_col(data_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for outliers in each column\n",
    "for column in data_scaled.select_dtypes(include=['float64', 'int64']).columns:\n",
    "    outliers = OutlierDetector(data_scaled[column])\n",
    "    if not outliers.empty:\n",
    "        print(f\"Outliers in {column}:\\n\", outliers)\n",
    "    else:\n",
    "        print(f\"No outliers detected in {column}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the correlation value between BNPL usage frequency and over-indebtedness\n",
    "correlation_matrix = data_scaled.corr()\n",
    "correlation_value = correlation_matrix.loc['bnpl_usage_frequency', 'over_indebtedness_flag']\n",
    "print(f\"Correlation between BNPL usage frequency and over-indebtedness: {correlation_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for outliers in the scaled data by splitting the data into two parts:\n",
    "# one with default_flag = 0 and one with default_flag = 1\n",
    "\n",
    "outlier_indices = []\n",
    "data_default_0 = data_scaled[data_scaled['default_flag'] == 0]\n",
    "data_default_1 = data_scaled[data_scaled['default_flag'] == 1]\n",
    "\n",
    "for column in continuous_cols:\n",
    "    outliers_0 = OutlierDetector(data_default_0[column])\n",
    "    outliers_1 = OutlierDetector(data_default_1[column])\n",
    "    \n",
    "    # Add indices of outliers to the list\n",
    "    outlier_indices.extend(outliers_0.index.tolist())\n",
    "    outlier_indices.extend(outliers_1.index.tolist())\n",
    "    \n",
    "    if not outliers_0.empty:\n",
    "        print(f\"Outliers in {column} for default_flag = 0:\\n\", outliers_0)\n",
    "    else:\n",
    "        print(f\"No outliers detected in {column} for default_flag = 0.\")\n",
    "    \n",
    "    if not outliers_1.empty:\n",
    "        print(f\"Outliers in {column} for default_flag = 1:\\n\", outliers_1)\n",
    "    else:\n",
    "        print(f\"No outliers detected in {column} for default_flag = 1.\")\n",
    "\n",
    "# Print the list of outlier indices\n",
    "print(\"Outlier indices:\", outlier_indices)\n",
    "print(\"Total number of outliers detected:\", len(outlier_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the outliers unwanted features from the data_scaled dataframe\n",
    "data_scaled_cleaned = data_scaled.drop(index=outlier_indices)\n",
    "data_scaled_cleaned = data_scaled_cleaned.drop(columns=['over_indebtedness_flag'])\n",
    "data_scaled_cleaned.to_csv('Tables/bnpl_scaled_cleaned.csv', index=False)\n",
    "data_scaled_cleaned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_cols = binary_col(data_scaled_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_scaled_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the feature engineering stress_usage_interaction = financial_stress_score × bnpl_usage_frequency\n",
    "data_scaled_cleaned['stress_usage_interaction'] = (\n",
    "    data_scaled_cleaned['financial_stress_score'] * data_scaled_cleaned['bnpl_usage_frequency']\n",
    ")\n",
    "data_engineered = data_scaled_cleaned.copy()\n",
    "data_engineered.to_csv('Tables/bnpl_engineered.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the mean and std of the new feature to ensure standardisation\n",
    "new_columns = [\n",
    "    'stress_usage_interaction',\n",
    "]\n",
    "mean_std = data_engineered[new_columns].agg(['mean', 'std'])\n",
    "print(\"Mean and Standard Deviation of New Columns:\")\n",
    "print(mean_std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for outliers in the new feature\n",
    "new_outlier_indices = []\n",
    "for column in new_columns:\n",
    "    outliers = OutlierDetector(data_engineered[column])\n",
    "    new_outlier_indices.extend(outliers.index.tolist())\n",
    "    if not outliers.empty:\n",
    "        print(f\"Outliers in {column}:\\n\", outliers)\n",
    "    else:\n",
    "        print(f\"No outliers detected in {column}.\")\n",
    "\n",
    "# Print the list of outlier indices\n",
    "print(\"New outlier indices:\", new_outlier_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove any duplicate outlier indices and sort them\n",
    "new_outlier_indices = sorted(set(new_outlier_indices))\n",
    "print(\"Unique sorted new outlier indices:\", new_outlier_indices)\n",
    "len(new_outlier_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate point-biserial correlation for new features with default_flag\n",
    "results = []\n",
    "for feat in new_columns:\n",
    "    r, p = pointbiserialr(data_scaled_cleaned[feat], data_scaled_cleaned['default_flag'])\n",
    "    results.append({'feature': feat, 'r': r, 'p_value': p})\n",
    "\n",
    "corr_df = pd.DataFrame(results).sort_values('r', key=abs, ascending=False)\n",
    "print(corr_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Day 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = pd.read_csv('Tables/bnpl_engineered.csv', index_col=0)\n",
    "\n",
    "# 1) split off test set\n",
    "train_val, test = train_test_split(\n",
    "    final_data,\n",
    "    test_size=0.20,\n",
    "    stratify=final_data['default_flag'],\n",
    "    random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) split train vs. validation from the remaining 80%\n",
    "train, val = train_test_split(\n",
    "    train_val,\n",
    "    test_size=0.25,                   \n",
    "    stratify=train_val['default_flag'],\n",
    "    random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Default rates:\",\n",
    "      train['default_flag'].mean(),\n",
    "      val['default_flag'].mean(),\n",
    "      test['default_flag'].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx = train.index.to_list()\n",
    "val_idx   = val.index.to_list()\n",
    "test_idx  = test.index.to_list()\n",
    "#print the indices and the number of indices in each set\n",
    "print(f\"Train indices: {train_idx[:10]}... ({len(train_idx)} total)\")\n",
    "print(f\"Validation indices: {val_idx[:10]}... ({len(val_idx)} total)\")\n",
    "print(f\"Test indices: {test_idx[:10]}... ({len(test_idx)} total)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create 3 csv files for train, val and test sets\n",
    "train.to_csv('Tables/bnpl_train.csv', index=False)\n",
    "val.to_csv('Tables/bnpl_val.csv', index=False)\n",
    "test.to_csv('Tables/bnpl_test.csv', index=False)\n",
    "\n",
    "print(f\"Train set shape: {train.shape}\")\n",
    "print(f\"Validation set shape: {val.shape}\")\n",
    "print(f\"Test set shape: {test.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
