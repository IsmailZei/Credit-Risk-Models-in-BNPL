{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pointbiserialr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OutlierDetector(data):\n",
    "    '''Detects outliers in a given pandas Series using the IQR method.\n",
    "    Args:\n",
    "        data (pd.Series): The input data series to check for outliers.\n",
    "    Returns:\n",
    "        pd.Series: A series containing the outliers.\n",
    "    Raises:\n",
    "        ValueError: If the input data is not a pandas Series.   \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    q1 = data.quantile(0.25)\n",
    "    q3 = data.quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    lower, upper = q1 - 1.5*iqr, q3 + 1.5*iqr\n",
    "    return data[(data < lower) | (data > upper)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DefaultFlagGenerator(df):\n",
    "    default_flag = (\n",
    "        (df['payment_delinquency_count'] >= 3).astype(int) +\n",
    "        (df['over_indebtedness_flag'] == 1).astype(int) +\n",
    "        (df['financial_stress_score'] >= 9).astype(int) +\n",
    "        (df['bnpl_debt_ratio'] >= 1.8).astype(int) +\n",
    "        (df['credit_limit_utilisation'] >= 95).astype(int)\n",
    "        ) >= 3 # Must meet at least 3 of the 5 conditions\n",
    "    df['default_flag'] = default_flag.astype(int)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_col(data):\n",
    "    #return the binary columns\n",
    "    binary_cols=[col for col in data.columns if data[col].nunique() == 2]\n",
    "    return binary_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def continuous_col(data):\n",
    "    #return the continuous columns\n",
    "    continuous_cols=[col for col in data.columns if data[col].nunique() > 2]\n",
    "    return continuous_cols\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Scaler(data):\n",
    "    binary_cols = binary_col(data)\n",
    "    continuous_cols = continuous_col(data)\n",
    "    scaler = StandardScaler()\n",
    "   # Scale the continuous columns\n",
    "    scaled_continuous = scaler.fit_transform(data[continuous_cols])\n",
    "    scaled_continuous_df = pd.DataFrame(scaled_continuous, columns=continuous_cols)\n",
    "    # Combine the scaled continuous columns with the binary columns\n",
    "    result = pd.concat([data[binary_cols].reset_index(drop=True), scaled_continuous_df.reset_index(drop=True)], axis=1)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open file bnpl\n",
    "data = pd.read_csv('bnpl.csv')\n",
    "data.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in data.select_dtypes(include=['float64', 'int64']).columns:\n",
    "    outliers = OutlierDetector(data[column])\n",
    "    print(f\"Outliers in {column}:\\n\", outliers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DefaultFlagGenerator(data)\n",
    "data_clean = data.drop(columns=['CustomerID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_scaled = Scaler(data_clean)\n",
    "data_scaled.to_csv('bnpl_scaled.csv', index=False)\n",
    "print(\"Data processing complete. Scaled data saved to 'bnpl_scaled.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run histograms for continuous columns\n",
    "continuous_cols = continuous_col(data_scaled)\n",
    "for column in continuous_cols:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(data_scaled[column], bins=30, edgecolor='black')\n",
    "    plt.title(f'Histogram of {column}')\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(axis='y', alpha=0.75)\n",
    "    plt.savefig(f'histograms/{column}_histogram.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for outliers in each column\n",
    "for column in data_scaled.select_dtypes(include=['float64', 'int64']).columns:\n",
    "    outliers = OutlierDetector(data_scaled[column])\n",
    "    if not outliers.empty:\n",
    "        print(f\"Outliers in {column}:\\n\", outliers)\n",
    "    else:\n",
    "        print(f\"No outliers detected in {column}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print boxplots for continuous columns\n",
    "for column in continuous_cols:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.boxplot(data_scaled[column])\n",
    "    plt.title(f'Boxplot of {column}')\n",
    "    plt.ylabel(column)\n",
    "    plt.grid(axis='y', alpha=0.75)\n",
    "    #save the boxplot as a png file in a folder named 'boxplots'\n",
    "    #plt.savefig(f'boxplots/{column}_boxplot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the balance of the binary columns\n",
    "binary_cols = binary_col(data_scaled)\n",
    "for column in binary_cols:\n",
    "    balance = data_scaled[column].value_counts(normalize=True)\n",
    "    print(f\"Balance of {column}:\\n\", balance)\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    balance.plot(kind='bar')\n",
    "    plt.title(f'Balance of {column}')\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel('Proportion')\n",
    "    plt.grid(axis='y', alpha=0.75)\n",
    "    plt.savefig(f'balances/{column}_balance.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the correlation matrix\n",
    "correlation_matrix = data_scaled.corr()\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.imshow(correlation_matrix, cmap='coolwarm', interpolation='none')\n",
    "plt.colorbar()\n",
    "plt.title('Correlation Matrix')\n",
    "#change the x and y ticks to the column names\n",
    "plt.xticks(ticks=np.arange(len(data_scaled.columns)), labels=data_scaled.columns, rotation=45, ha='right')\n",
    "plt.yticks(ticks=np.arange(len(data_scaled.columns)), labels=data_scaled.columns)\n",
    "plt.savefig('correlation_matrix.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store outlier indices\n",
    "outlier_indices = []\n",
    "\n",
    "# Check for outliers in the scaled data by splitting the data into two parts: one with default_flag = 0 and one with default_flag = 1\n",
    "data_default_0 = data_scaled[data_scaled['default_flag'] == 0]\n",
    "data_default_1 = data_scaled[data_scaled['default_flag'] == 1]\n",
    "\n",
    "for column in continuous_cols:\n",
    "    outliers_0 = OutlierDetector(data_default_0[column])\n",
    "    outliers_1 = OutlierDetector(data_default_1[column])\n",
    "    \n",
    "    # Add indices of outliers to the list\n",
    "    outlier_indices.extend(outliers_0.index.tolist())\n",
    "    outlier_indices.extend(outliers_1.index.tolist())\n",
    "    \n",
    "    if not outliers_0.empty:\n",
    "        print(f\"Outliers in {column} for default_flag = 0:\\n\", outliers_0)\n",
    "    else:\n",
    "        print(f\"No outliers detected in {column} for default_flag = 0.\")\n",
    "    \n",
    "    if not outliers_1.empty:\n",
    "        print(f\"Outliers in {column} for default_flag = 1:\\n\", outliers_1)\n",
    "    else:\n",
    "        print(f\"No outliers detected in {column} for default_flag = 1.\")\n",
    "\n",
    "# Print the list of outlier indices\n",
    "print(\"Outlier indices:\", outlier_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the outliers from the data_scaled dataframe\n",
    "data_scaled_cleaned = data_scaled.drop(index=outlier_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot 2 boxplots for each column one with default flag 0 and one with default flag 1\n",
    "continuous_cols = continuous_col(data_scaled_cleaned)\n",
    "for column in continuous_cols:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    data_scaled_cleaned.boxplot(column=column, by='default_flag')\n",
    "    plt.title(f'Boxplot of {column} by Default Flag')\n",
    "    plt.suptitle('')\n",
    "    plt.xlabel('Default Flag')\n",
    "    plt.ylabel(column)\n",
    "    plt.grid(axis='y', alpha=0.75)\n",
    "    plt.savefig(f'boxplots/{column}_boxplot_by_default_flag.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the correlation matrix again after removing outliers\n",
    "correlation_matrix_cleaned = data_scaled_cleaned.corr()\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.imshow(correlation_matrix_cleaned, cmap='coolwarm', interpolation='none')\n",
    "plt.colorbar()\n",
    "plt.title('Correlation Matrix (Cleaned Data)')\n",
    "#change the x and y ticks to the column names\n",
    "plt.xticks(ticks=np.arange(len(data_scaled_cleaned.columns)), labels=data_scaled_cleaned.columns, rotation=45, ha='right')\n",
    "plt.yticks(ticks=np.arange(len(data_scaled_cleaned.columns)), labels=data_scaled_cleaned.columns)\n",
    "plt.savefig('correlation_matrix_cleaned.png')\n",
    "\n",
    "#what is the numerical value of the corr of 'bnpl_usage_frequency' and 'over_indebtedness_flag'\n",
    "corr_value = correlation_matrix_cleaned.loc['bnpl_usage_frequency', 'over_indebtedness_flag']\n",
    "print(f\"Correlation between 'bnpl_usage_frequency' and 'over_indebtedness_flag': {corr_value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the over_indebtedness_flag column from the data_scaled_cleaned dataframe\n",
    "data_scaled_cleaned = data_scaled_cleaned.drop(columns=['over_indebtedness_flag'])\n",
    "binary_cols = [col for col in binary_cols if col != 'over_indebtedness_flag']\n",
    "data_scaled_cleaned.to_csv('bnpl_scaled_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the balance of the binary columns again after removing outliers\n",
    "binary_cols = binary_col(data_scaled_cleaned)\n",
    "for column in binary_cols:\n",
    "    balance = data_scaled_cleaned[column].value_counts(normalize=True)\n",
    "    print(f\"Balance of {column} after cleaning:\\n\", balance)\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    balance.plot(kind='bar')\n",
    "    plt.title(f'Balance of {column} after cleaning')\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel('Proportion')\n",
    "    plt.grid(axis='y', alpha=0.75)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_scaled_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat the following stress_usage_interaction = financial_stress_score × bnpl_usage_frequency\n",
    "data_scaled_cleaned['stress_usage_interaction'] = (\n",
    "    data_scaled_cleaned['financial_stress_score'] * data_scaled_cleaned['bnpl_usage_frequency']\n",
    ")\n",
    "\n",
    "#adjusted_debt_interaction = bnpl_debt_ratio + financial_stress_score\n",
    "data_scaled_cleaned['adjusted_debt_interaction'] = (\n",
    "    data_scaled_cleaned['bnpl_debt_ratio'] * data_scaled_cleaned['financial_stress_score']\n",
    ")\n",
    "data_scaled_cleaned.to_csv('bnpl_scaled_cleaned_interactions.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the mean and std of the new columns\n",
    "new_columns = [\n",
    "    'stress_usage_interaction',\n",
    "    'adjusted_debt_interaction'\n",
    "]\n",
    "mean_std = data_scaled_cleaned[new_columns].agg(['mean', 'std'])\n",
    "print(\"Mean and Standard Deviation of New Columns:\")\n",
    "print(mean_std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store outlier indices for new columns\n",
    "new_outlier_indices = []\n",
    "for column in new_columns:\n",
    "    outliers = OutlierDetector(data_scaled_cleaned[column])\n",
    "    new_outlier_indices.extend(outliers.index.tolist())\n",
    "    if not outliers.empty:\n",
    "        print(f\"Outliers in {column}:\\n\", outliers)\n",
    "    else:\n",
    "        print(f\"No outliers detected in {column}.\")\n",
    "\n",
    "# Print the list of outlier indices\n",
    "print(\"New outlier indices:\", new_outlier_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove any duplicate outlier indices and sort them\n",
    "new_outlier_indices = sorted(set(new_outlier_indices))\n",
    "print(\"Unique sorted new outlier indices:\", new_outlier_indices)\n",
    "len(new_outlier_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the box plots for the new columns\n",
    "for column in new_columns:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    data_scaled_cleaned.boxplot(column=column, by='default_flag')\n",
    "    plt.title(f'Boxplot of {column} by Default Flag')\n",
    "    plt.suptitle('')\n",
    "    plt.xlabel('Default Flag')\n",
    "    plt.ylabel(column)\n",
    "    plt.grid(axis='y', alpha=0.75)\n",
    "    plt.savefig(f'boxplots/{column}_boxplot_by_default_flag.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = []\n",
    "for feat in new_columns:\n",
    "    r, p = pointbiserialr(data_scaled_cleaned[feat], data_scaled_cleaned['default_flag'])\n",
    "    results.append({'feature': feat, 'r': r, 'p_value': p})\n",
    "\n",
    "corr_df = pd.DataFrame(results).sort_values('r', key=abs, ascending=False)\n",
    "print(corr_df)\n",
    "# save the data-s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot a new correlation matrix with the new columns\n",
    "correlation_matrix_new = data_scaled_cleaned.corr()\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.imshow(correlation_matrix_new, cmap='coolwarm', interpolation='none')\n",
    "plt.colorbar()\n",
    "plt.title('Correlation Matrix with New Columns')\n",
    "#change the x and y ticks to the column names\n",
    "plt.xticks(ticks=np.arange(len(data_scaled_cleaned.columns)), labels=data_scaled_cleaned.columns, rotation=45, ha='right')\n",
    "plt.yticks(ticks=np.arange(len(data_scaled_cleaned.columns)), labels=data_scaled_cleaned.columns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Day 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = pd.read_csv('bnpl_scaled_cleaned_interactions.csv')\n",
    "\n",
    "# 1) split off test set\n",
    "train_val, test = train_test_split(\n",
    "    final_data,\n",
    "    test_size=0.20,\n",
    "    stratify=final_data['default_flag'],\n",
    "    random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2) split train vs. validation from the remaining 80%\n",
    "train, val = train_test_split(\n",
    "    train_val,\n",
    "    test_size=0.25,                       # 0.25 × 80% → 20% overall\n",
    "    stratify=train_val['default_flag'],\n",
    "    random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Default rates:\",\n",
    "      train['default_flag'].mean(),\n",
    "      val['default_flag'].mean(),\n",
    "      test['default_flag'].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx = train.index.to_list()\n",
    "val_idx   = val.index.to_list()\n",
    "test_idx  = test.index.to_list()\n",
    "#print the indices and the number of indices in each set\n",
    "print(f\"Train indices: {train_idx[:10]}... ({len(train_idx)} total)\")\n",
    "print(f\"Validation indices: {val_idx[:10]}... ({len(val_idx)} total)\")\n",
    "print(f\"Test indices: {test_idx[:10]}... ({len(test_idx)} total)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create 3 csv files for train, val and test sets\n",
    "train.to_csv('train_set.csv', index=False)\n",
    "val.to_csv('val_set.csv', index=False)\n",
    "test.to_csv('test_set.csv', index=False)\n",
    "# Print the shape of each set\n",
    "print(f\"Train set shape: {train.shape}\")\n",
    "print(f\"Validation set shape: {val.shape}\")\n",
    "print(f\"Test set shape: {test.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
